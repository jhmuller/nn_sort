{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5gZ8iFtRL+fY8rV0/UrR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmuller/nn_sort/blob/main/sorting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teachine a Neural Net to Sort\n"
      ],
      "metadata": {
        "id": "nUUV0oIq3gEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teachine a Neural Net to Sort\n",
        "Can I make a Neural Net that can sort?\n",
        "It seemed like an interesting question to me.\n",
        "\n",
        "The first challenge is how to model the data.\n",
        "I am sorting integers and the net outputs floats.\n",
        "I did not think that rounding and comparing\n",
        "the results to the true value would work well.\n",
        "\n",
        "So I devised a way to model the problem using \n",
        "the moves that bubble sort would make when \n",
        "sorting the list.  Each move is encoded\n",
        "as a vector of length N, where N is the length \n",
        "of the input list. All the entries in the vector \n",
        "are 0 except the one that tells the position\n",
        "of the left number to be swapped in this step of Bubble sort.  There are at most N*(N-1)/2 steps.\n",
        "Once the steps are done, i.e. the list is sorted  subsequent steps are encoded with a 1 in the last position indicating Noop or Nothing to do.\n",
        "\n",
        "The Net will output probabilities for each vector.\n",
        "Then I will compare the predicted values to the actual and that will be the loss.\n",
        "\n",
        "Let's see if it works."
      ],
      "metadata": {
        "id": "8tDGUANf3oo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2dxhwj6OG-KG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import math\n",
        "from itertools import permutations\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the input data.\n"
      ],
      "metadata": {
        "id": "4FaU_TYq5YZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def sort_seq(x):\n",
        "  lst = x.copy()\n",
        "  N = len(lst)\n",
        "  swaped = True\n",
        "  seq = []\n",
        "  while swaped:\n",
        "    swaped = False\n",
        "    for i in range(N-1):\n",
        "      if lst[i] > lst[i+1]:\n",
        "        lst[i], lst[i+1] = lst[i+1], lst[i]\n",
        "        seq.append(i)\n",
        "        swaped = True\n",
        "  while len(seq) < N*(N-1)/2:\n",
        "    seq.append(N-1)\n",
        "  return lst, seq\n",
        "\n",
        "def apply_seq(x_in, seq):\n",
        "  x_out = x_in.copy()\n",
        "  N = len(x_in)\n",
        "  for i in seq:\n",
        "    if i >= N-1:\n",
        "      continue\n",
        "    x_out[i], x_out[i+1] = x_out[i+1], x_out[i]\n",
        "  return x_out\n",
        "\n",
        "def lst_diff(true, pred):\n",
        "  diff = 0\n",
        "  for i, p in enumerate(pred):\n",
        "    diff += abs(p - true[i])\n",
        "  return diff\n",
        "\n",
        "def factorial(n):\n",
        "  res = n\n",
        "  for i in range(2, n):\n",
        "    res = res*i\n",
        "  return res\n",
        "\n",
        "def generate_input_data(m=5):\n",
        "  res = []\n",
        "  w = int((m*(m-1))/2)\n",
        "  first = list(range(m))\n",
        "  iter = permutations(first)\n",
        "  for i, x in enumerate(iter):\n",
        "    _, y = sort_seq(list(x))\n",
        "    mat = np.zeros((w, m))\n",
        "    for j, p in enumerate(y):\n",
        "      mat[j, p] = 1\n",
        "    item = {}\n",
        "    item[\"list\"] = torch.Tensor(x)\n",
        "    item[\"mat\"] = mat\n",
        "    res.append(item)\n",
        "  print(f\" # items: {len(res)}\")\n",
        "  return res\n",
        "\n",
        "\n",
        "class SortDataset(Dataset):\n",
        "    \"\"\"Sort dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return(self.data[idx])\n",
        "\n",
        "sdata = generate_input_data(m=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LrBdtY_5Imi",
        "outputId": "b3df00c9-c878-4bc3-b1f9-6d5026c10ba4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " # items: 40320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into Train and Test\n",
        "I put 80% of the permutations in train and the rest in test.  I do this simply by taking the first 80% generated above for train and the rest for test.\n",
        "\n",
        "Also here I create the train and test dataloaders."
      ],
      "metadata": {
        "id": "kD8J8Xp165Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(sdata)\n",
        "cutoff = int(N*.8)\n",
        "train = sdata[:cutoff]\n",
        "test = sdata[cutoff:]\n",
        "train_dataset = SortDataset(train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "test_dataset = SortDataset(test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "hhqJgzzY95q0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Net\n",
        "I will use 2 hidden, linear, layers of size 100.\n",
        "\n",
        "I reshape the output into a matrix and do softmax on the rows to make probabilities.\n"
      ],
      "metadata": {
        "id": "_xayeBjB7cDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SortNet(nn.Module):\n",
        "    def __init__(self, lst_len, hidden=100):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.lst_len = lst_len\n",
        "        self.seq_len = int((lst_len * (lst_len-1))/2)\n",
        "        out_len = (lst_len) * self.seq_len        \n",
        "        print(lst_len, out_len)\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(lst_len, hidden)        \n",
        "        self.linear2 = nn.Linear(hidden, out_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "            \"\"\"\n",
        "            \"\"\"\n",
        "            h_relu = F.relu(self.linear1(x))\n",
        "            y = torch.sigmoid(self.linear2(h_relu))\n",
        "            y_pred = y.view((y.shape[0], self.seq_len , self.lst_len))\n",
        "            y_pred = torch.softmax(y_pred, dim=2)\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "B2t1U52P-EaB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop\n",
        "Nothing too unusual here, mostly a standard torch train loop.\n",
        "\n",
        "I do have to convert the input to tensors and to float32."
      ],
      "metadata": {
        "id": "tb2ajeLN8QfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_freq = 20\n",
        "model = SortNet(8)\n",
        "loss_fn = nn.MSELoss() # nn.L1Loss() # \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "for ei in range(121):\n",
        "  model.train()\n",
        "  for bi, sample in enumerate(train_dataloader):\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "    X = X.to(torch.float32)\n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "    y_pred = model(X)          \n",
        "\n",
        "    optimizer.zero_grad()       \n",
        "    loss = loss_fn(y, y_pred)  \n",
        "    loss.backward()        \n",
        "    optimizer.step()\n",
        "  if ei % print_freq == 0: \n",
        "    print(f\"epoch {ei} batch {bi} loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvwfWrbDXR6",
        "outputId": "701de0e4-b5f0-4cda-f82d-7d7993f78ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 224\n",
            "epoch 0 batch 8063 loss 0.08435816317796707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  sum_loss = 0\n",
        "  model.eval()\n",
        "  for bi, sample in enumerate(test_dataloader):\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "    X = X.to(torch.float32) \n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "    y_pred = model(X)           # compute model output\n",
        "    if ei == 50 and (bi % print_freq == 0):\n",
        "      pass     \n",
        "    loss = loss_fn(y, y_pred)  # calculate loss\n",
        "    sum_loss += loss\n",
        "    if bi % (print_freq*10) == 0: \n",
        "      print(f\"batch {bi} loss {loss} sum_loss {sum_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PC4LLPbVn__",
        "outputId": "12c7fc12-0a30-4e19-dfd8-d72fa41b18a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.09204287081956863 sum_loss 0.09204287081956863\n",
            "batch 200 loss 0.08954490721225739 sum_loss 18.215654373168945\n",
            "batch 400 loss 0.10235906392335892 sum_loss 36.236358642578125\n",
            "batch 600 loss 0.08598863333463669 sum_loss 54.36198043823242\n",
            "batch 800 loss 0.08811386674642563 sum_loss 72.5230484008789\n",
            "batch 1000 loss 0.09514040499925613 sum_loss 90.70939636230469\n",
            "batch 1200 loss 0.091006800532341 sum_loss 108.84637451171875\n",
            "batch 1400 loss 0.0850461795926094 sum_loss 126.89704132080078\n",
            "batch 1600 loss 0.09272421896457672 sum_loss 145.00184631347656\n",
            "batch 1800 loss 0.08763337880373001 sum_loss 163.06765747070312\n",
            "batch 2000 loss 0.09209877252578735 sum_loss 181.2366180419922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.Tensor([4, 3, 2, 1])\n",
        "test = test.unsqueeze(0)\n",
        "sample = test_dataset[0]\n",
        "X = sample[\"list\"]\n",
        "X = X.unsqueeze(0)\n",
        "res = model(X)\n",
        "print(X)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3JCHeJRrvZ",
        "outputId": "700bccf5-de63-4861-e889-2d17d91c1ed6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 2., 5., 7., 0., 1., 3., 4.]])\n",
            "tensor([[[0.2797, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029],\n",
            "         [0.1028, 0.2795, 0.1028, 0.1034, 0.1028, 0.1028, 0.1028, 0.1028],\n",
            "         [0.1030, 0.1030, 0.1030, 0.2783, 0.1040, 0.1030, 0.1030, 0.1030],\n",
            "         [0.1024, 0.1024, 0.1024, 0.1024, 0.2783, 0.1073, 0.1024, 0.1024],\n",
            "         [0.1024, 0.1024, 0.1024, 0.1024, 0.1024, 0.2785, 0.1069, 0.1024],\n",
            "         [0.0991, 0.0991, 0.1362, 0.0991, 0.0991, 0.0991, 0.2693, 0.0991],\n",
            "         [0.1010, 0.1010, 0.2741, 0.1200, 0.1010, 0.1010, 0.1010, 0.1010],\n",
            "         [0.1026, 0.1026, 0.1027, 0.2788, 0.1054, 0.1026, 0.1026, 0.1026],\n",
            "         [0.0866, 0.0868, 0.0866, 0.0984, 0.2334, 0.2349, 0.0866, 0.0866],\n",
            "         [0.0824, 0.1417, 0.0824, 0.0824, 0.2238, 0.2226, 0.0824, 0.0824],\n",
            "         [0.0874, 0.2376, 0.2376, 0.0875, 0.0878, 0.0874, 0.0874, 0.0874],\n",
            "         [0.1631, 0.1640, 0.1640, 0.1640, 0.1640, 0.0603, 0.0603, 0.0603],\n",
            "         [0.1638, 0.1639, 0.1639, 0.1639, 0.1637, 0.0603, 0.0603, 0.0603],\n",
            "         [0.1651, 0.1651, 0.1651, 0.1651, 0.1573, 0.0607, 0.0607, 0.0607],\n",
            "         [0.1891, 0.1894, 0.1894, 0.1535, 0.0697, 0.0697, 0.0697, 0.0697],\n",
            "         [0.1864, 0.1895, 0.1823, 0.1621, 0.0697, 0.0697, 0.0697, 0.0704],\n",
            "         [0.1724, 0.1896, 0.1215, 0.0850, 0.0755, 0.0755, 0.0755, 0.2051],\n",
            "         [0.1027, 0.1066, 0.1027, 0.1024, 0.1024, 0.1024, 0.1024, 0.2783],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797],\n",
            "         [0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.1029, 0.2797]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(res, dim=2)\n",
        "torch.argmax(res, dim=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArbZvT2yIOSE",
        "outputId": "d852d57b-6fac-464e-a0d4-119236c21d5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1, 3, 4, 5, 6, 2, 3, 5, 4, 2, 2, 1, 1, 1, 1, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "         7, 7, 7, 7]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Square(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_times_x= torch.mm(x, self.weights.t())\n",
        "        # \n",
        "        print(f\" x shape {x.shape} weights shape {self.weights.shape} res shape {w_timex_x.shape}\")\n",
        "        return torch.add(w_times_x, self.bias) "
      ],
      "metadata": {
        "id": "QTATN8dJHgo3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSgIJZTFLBjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}