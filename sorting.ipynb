{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMevwRyCrQftxrdkGNKtE3V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmuller/nn_sort/blob/main/sorting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Teachine a Neural Net to Sort\n"
      ],
      "metadata": {
        "id": "nUUV0oIq3gEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Teachine a Neural Net to Sort\n",
        "Can I make a Neural Net that can sort?\n",
        "It seemed like an interesting question to me.\n",
        "\n",
        "The first challenge is how to model the data.\n",
        "I am sorting integers and the net outputs floats.\n",
        "I did not think that rounding and comparing\n",
        "the results to the true value would work well.\n",
        "\n",
        "So I devised a way to model the problem using \n",
        "the moves that bubble sort would make when \n",
        "sorting the list.  Each move is encoded\n",
        "as a vector of length N, where N is the length \n",
        "of the input list. All the entries in the vector \n",
        "are 0 except the one that tells the position\n",
        "of the left number to be swapped in this step of Bubble sort.  There are at most N*(N-1)/2 steps.\n",
        "Once the steps are done, i.e. the list is sorted  subsequent steps are encoded with a 1 in the last position indicating Noop or Nothing to do.\n",
        "\n",
        "The Net will output probabilities for each vector.\n",
        "Then I will compare the predicted values to the actual and that will be the loss.\n",
        "\n",
        "Let's see if it works."
      ],
      "metadata": {
        "id": "8tDGUANf3oo3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2dxhwj6OG-KG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import math\n",
        "from itertools import permutations\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the input data.\n"
      ],
      "metadata": {
        "id": "4FaU_TYq5YZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_LEN = 7\n",
        "\n",
        "def sort_seq(x):\n",
        "  lst = x.copy()\n",
        "  N = len(lst)\n",
        "  swaped = True\n",
        "  seq = []\n",
        "  while swaped:\n",
        "    swaped = False\n",
        "    for i in range(N-1):\n",
        "      if lst[i] > lst[i+1]:\n",
        "        lst[i], lst[i+1] = lst[i+1], lst[i]\n",
        "        seq.append(i)\n",
        "        swaped = True\n",
        "  while len(seq) < N*(N-1)/2:\n",
        "    seq.append(N-1)\n",
        "  return lst, seq\n",
        "\n",
        "def apply_seq(x_in, seq):\n",
        "  x_out = x_in.copy()\n",
        "  N = len(x_in)\n",
        "  for i in seq:\n",
        "    if i >= N-1:\n",
        "      continue\n",
        "    x_out[i], x_out[i+1] = x_out[i+1], x_out[i]\n",
        "  return x_out\n",
        "\n",
        "def lst_diff(true, pred):\n",
        "  diff = 0\n",
        "  for i, p in enumerate(pred):\n",
        "    diff += abs(p - true[i])\n",
        "  return diff\n",
        "\n",
        "def factorial(n):\n",
        "  res = n\n",
        "  for i in range(2, n):\n",
        "    res = res*i\n",
        "  return res\n",
        "\n",
        "def generate_input_data(m=5):\n",
        "  res = []\n",
        "  w = int((m*(m-1))/2)\n",
        "  first = list(range(m))\n",
        "  iter = permutations(first)\n",
        "  for i, x in enumerate(iter):\n",
        "    _, y = sort_seq(list(x))\n",
        "    mat = np.zeros((w, m))\n",
        "    for j, p in enumerate(y):\n",
        "      mat[j, p] = 1\n",
        "    item = {}\n",
        "    item[\"list\"] = torch.Tensor(x)\n",
        "    item[\"mat\"] = mat\n",
        "    res.append(item)\n",
        "  print(f\" # items: {len(res)}\")\n",
        "  return res\n",
        "\n",
        "\n",
        "class SortDataset(Dataset):\n",
        "    \"\"\"Sort dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return(self.data[idx])\n",
        "\n",
        "sdata = generate_input_data(m=LIST_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "-LrBdtY_5Imi",
        "outputId": "e11bc9a8-a764-4b5c-c0d6-5c73ebad23d6"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-53f277f90de6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0msdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_input_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLIST_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-129-53f277f90de6>\u001b[0m in \u001b[0;36mgenerate_input_data\u001b[0;34m(m)\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m       \u001b[0mmat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'zeros'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sort_seq([3, 4, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm5ft4ij7aN0",
        "outputId": "a79807a1-1fc9-45c9-ecd8-cdba1f8de307"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0, 3, 4], [1, 0, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split into Train and Test\n",
        "I put 80% of the permutations in train and the rest in test.  I do this simply by taking the first 80% generated above for train and the rest for test.\n",
        "\n",
        "Also here I create the train and test dataloaders."
      ],
      "metadata": {
        "id": "kD8J8Xp165Xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N = len(sdata)\n",
        "cutoff = int(N*.8)\n",
        "train = sdata[:cutoff]\n",
        "test = sdata[cutoff:]\n",
        "train_dataset = SortDataset(train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "test_dataset = SortDataset(test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "hhqJgzzY95q0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Net\n",
        "I will use 2 hidden, linear, layers of size 100.\n",
        "\n",
        "I reshape the output into a matrix and do softmax on the rows to make probabilities.\n"
      ],
      "metadata": {
        "id": "_xayeBjB7cDs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SortNet(nn.Module):\n",
        "    def __init__(self, lst_len, num_hidden_layers = 10, hidden_size=200):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.lst_len = lst_len\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.seq_len = int((lst_len * (lst_len-1))/2)\n",
        "        out_len = (lst_len) * self.seq_len        \n",
        "        print(lst_len, out_len)\n",
        "        super().__init__()\n",
        "        self.input = nn.Linear(lst_len, hidden_size)   \n",
        "        self.hidden_layers = []\n",
        "        for i in range(num_hidden_layers):\n",
        "          self.hidden_layers.append(nn.Linear(hidden_size, hidden_size))                                  \n",
        "        self.output = nn.Linear(hidden_size, out_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "            \"\"\"\n",
        "            \"\"\"\n",
        "            y = F.leaky_relu(self.input(x))\n",
        "            for i in range(self.num_hidden_layers):\n",
        "              y = F.leaky_relu(self.hidden_layers[i](y))           \n",
        "            y = (self.output(y))\n",
        "            y_pred = y.view((y.shape[0], self.seq_len , self.lst_len))\n",
        "            y_pred = torch.softmax(y_pred, dim=2)\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "B2t1U52P-EaB"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train loop\n",
        "Nothing too unusual here, mostly a standard torch train loop.\n",
        "\n",
        "I do have to convert the input to tensors and to float32."
      ],
      "metadata": {
        "id": "tb2ajeLN8QfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_freq = 20\n",
        "model = SortNet(LIST_LEN, num_hidden_layers=10, hidden_size=100)\n",
        "loss_fn =  nn.MSELoss() # nn.L1Loss() #\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005)\n",
        "for ei in range(181):\n",
        "  model.train()\n",
        "  for bi, sample in enumerate(train_dataloader):\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "    X = X.to(torch.float32)\n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "    y_pred = model(X)          \n",
        "\n",
        "    optimizer.zero_grad()       \n",
        "    loss = loss_fn(y, y_pred)  \n",
        "    #if ei % print_freq == 0: \n",
        "    # print(f\"epoch {ei} batch {bi} loss {loss}\")\n",
        "\n",
        "    loss.backward()        \n",
        "    optimizer.step()\n",
        "  if ei % print_freq == 0: \n",
        "    print(f\"epoch {ei} batch {bi} loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvwfWrbDXR6",
        "outputId": "b98743b6-ebdb-4b91-9b14-faf0db1abdc0"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 90\n",
            "epoch 0 batch 143 loss 0.06909219920635223\n",
            "epoch 20 batch 143 loss 0.0753786638379097\n",
            "epoch 40 batch 143 loss 0.06060944125056267\n",
            "epoch 60 batch 143 loss 0.06364498287439346\n",
            "epoch 80 batch 143 loss 0.06556541472673416\n",
            "epoch 100 batch 143 loss 0.06278382986783981\n",
            "epoch 120 batch 143 loss 0.044715266674757004\n",
            "epoch 140 batch 143 loss 0.0627627745270729\n",
            "epoch 160 batch 143 loss 0.03545656055212021\n",
            "epoch 180 batch 143 loss 0.03620775416493416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  sum_loss = 0\n",
        "  model.eval()\n",
        "  for bi, sample in enumerate(test_dataloader):\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "    X = X.to(torch.float32) \n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "    y_pred = model(X)           # compute model output\n",
        "    if ei == 50 and (bi % print_freq == 0):\n",
        "      pass     \n",
        "    loss = loss_fn(y, y_pred)  # calculate loss\n",
        "    sum_loss += loss\n",
        "    if bi % (print_freq*10) == 0: \n",
        "      print(f\"batch {bi} loss {loss} sum_loss {sum_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PC4LLPbVn__",
        "outputId": "c46d0eda-e4fc-4b58-80ab-7158877d6997"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.11207365244626999 sum_loss 0.11207365244626999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.Tensor([1, 2, 4, 3])\n",
        "test = test.unsqueeze(0)\n",
        "sample = test_dataset[10]\n",
        "X = sample[\"list\"]\n",
        "print(X)\n",
        "X = X.unsqueeze(0)\n",
        "res = model(X)\n",
        "np = X.numpy()\n",
        "print(np)\n",
        "lst = list(np[0])\n",
        "print(type(lst))\n",
        "print(lst)\n",
        "sorted, true_seq = sort_seq(lst)\n",
        "print(true_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3JCHeJRrvZ",
        "outputId": "31c6744c-5bbd-41b3-9bd6-b3e48daa04a7"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([4., 5., 1., 3., 0., 2.])\n",
            "[[4. 5. 1. 3. 0. 2.]]\n",
            "<class 'list'>\n",
            "[4.0, 5.0, 1.0, 3.0, 0.0, 2.0]\n",
            "[1, 2, 3, 4, 0, 1, 2, 3, 1, 2, 0, 5, 5, 5, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(res, dim=2)\n",
        "am = torch.argmax(res, dim=2)\n",
        "print(am)\n",
        "pred_seq = list(am.numpy()[0])\n",
        "print(lst, pred_seq)\n",
        "apply_seq(lst, true_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArbZvT2yIOSE",
        "outputId": "406e8d5b-d387-4829-ab08-7c161f640a2f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 2, 3, 4, 0, 2, 3, 1, 2, 1, 0, 5, 5, 5, 5]])\n",
            "[4.0, 5.0, 1.0, 3.0, 0.0, 2.0] [0, 2, 3, 4, 0, 2, 3, 1, 2, 1, 0, 5, 5, 5, 5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Square(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_times_x= torch.mm(x, self.weights.t())\n",
        "        # \n",
        "        print(f\" x shape {x.shape} weights shape {self.weights.shape} res shape {w_timex_x.shape}\")\n",
        "        return torch.add(w_times_x, self.bias) "
      ],
      "metadata": {
        "id": "QTATN8dJHgo3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSgIJZTFLBjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}