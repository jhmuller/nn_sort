{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEpn052m+qEea0oOXHG/6e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhmuller/nn_sort/blob/main/sorting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2dxhwj6OG-KG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "import math\n",
        "from itertools import permutations\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def sort_seq(x):\n",
        "  lst = x.copy()\n",
        "  N = len(lst)\n",
        "  swaped = True\n",
        "  seq = []\n",
        "  while swaped:\n",
        "    swaped = False\n",
        "    for i in range(N-1):\n",
        "      if lst[i] > lst[i+1]:\n",
        "        lst[i], lst[i+1] = lst[i+1], lst[i]\n",
        "        seq.append(i)\n",
        "        swaped = True\n",
        "  while len(seq) < N*(N-1)/2:\n",
        "    seq.append(N-1)\n",
        "  return lst, seq\n",
        "\n",
        "def apply_seq(x_in, seq):\n",
        "  x_out = x_in.copy()\n",
        "  N = len(x_in)\n",
        "  for i in seq:\n",
        "    if i >= N-1:\n",
        "      continue\n",
        "    x_out[i], x_out[i+1] = x_out[i+1], x_out[i]\n",
        "  return x_out\n",
        "\n",
        "def lst_diff(true, pred):\n",
        "  diff = 0\n",
        "  for i, p in enumerate(pred):\n",
        "    diff += abs(p - true[i])\n",
        "  return diff\n",
        "\n",
        "def factorial(n):\n",
        "  res = n\n",
        "  for i in range(2, n):\n",
        "    res = res*i\n",
        "  return res\n",
        "\n",
        "def generate_input_data(m=5):\n",
        "  res = []\n",
        "  w = int((m*(m-1))/2)\n",
        "  first = list(range(m))\n",
        "  iter = permutations(first)\n",
        "  for i, x in enumerate(iter):\n",
        "    _, y = sort_seq(list(x))\n",
        "    mat = np.zeros((w, m))\n",
        "    for j, p in enumerate(y):\n",
        "      mat[j, p] = 1\n",
        "    item = {}\n",
        "    item[\"list\"] = torch.Tensor(x)\n",
        "    item[\"mat\"] = mat\n",
        "    res.append(item)\n",
        "  print(f\" # items: {len(res)}\")\n",
        "  return res\n",
        "\n",
        "\n",
        "class SortDataset(Dataset):\n",
        "    \"\"\"Sort dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return(self.data[idx])\n",
        "\n",
        "sdata = generate_input_data(m=8)\n",
        "N = len(sdata)\n",
        "cutoff = int(N*.8)\n",
        "train = sdata[:cutoff]\n",
        "test = sdata[cutoff:]\n",
        "train_dataset = SortDataset(train)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "test_dataset = SortDataset(test)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LrBdtY_5Imi",
        "outputId": "5a18691a-ee99-4f28-fce3-f75722533b90"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " # items: 40320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)\n",
        "len(test)\n",
        "factorial(8)\n",
        "train[3]\n",
        "next(iter(train_dataloader))\n",
        "train_dataset[2]\n",
        "for i in [1, 2]:\n",
        "  print(next(iter(train_dataloader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhqJgzzY95q0",
        "outputId": "6d73ead2-9e17-4d9e-9cd0-59e65a185484"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'list': tensor([[1., 5., 2., 3., 6., 4., 0., 7.]]), 'mat': tensor([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=torch.float64)}\n",
            "{'list': tensor([[1., 5., 3., 2., 4., 7., 0., 6.]]), 'mat': tensor([[[0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1.]]], dtype=torch.float64)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class SortNet(nn.Module):\n",
        "    def __init__(self, lst_len, hidden=100):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        self.lst_len = lst_len\n",
        "        self.seq_len = int((lst_len * (lst_len-1))/2)\n",
        "        out_len = (lst_len) * self.seq_len        \n",
        "        print(lst_len, out_len)\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(lst_len, hidden) \n",
        "        self.linear2 = nn.Linear(hidden, hidden)        \n",
        "        self.linear3 = nn.Linear(hidden, out_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "            \"\"\"\n",
        "            In the forward function we accept a Variable of input data and we must \n",
        "            return a Variable of output data. We can use Modules defined in the \n",
        "            constructor as well as arbitrary operators on Variables.\n",
        "            \"\"\"\n",
        "            h_relu = F.relu(self.linear1(x))\n",
        "            y = torch.sigmoid(self.linear3(h_relu))\n",
        "            y_pred = y.view((y.shape[0], self.seq_len , self.lst_len))\n",
        "            y_pred = torch.softmax(y_pred, dim=2)\n",
        "            return y_pred"
      ],
      "metadata": {
        "id": "B2t1U52P-EaB"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_freq = 20\n",
        "model = SortNet(8)\n",
        "loss_fn = nn.MSELoss() #nn.BCELoss() # CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "for ei in range(101):\n",
        "  #print(f\"epoch {ei}\")\n",
        "  for bi, sample in enumerate(train_dataloader):\n",
        "    #print(sample, \"----\")\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "\n",
        "    X = X.to(torch.float32)\n",
        " \n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "\n",
        "    y_pred = model(X)           # compute model output\n",
        "    if ei == 50 and (bi % print_freq == 0):\n",
        "      pass\n",
        "    optimizer.zero_grad()       \n",
        "    loss = loss_fn(y, y_pred)  \n",
        "\n",
        "    loss.backward()        \n",
        "    optimizer.step()\n",
        "  if ei % print_freq == 0: \n",
        "    print(f\"epoch {ei} batch {bi} loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYvwfWrbDXR6",
        "outputId": "32006b0f-ce8a-4a8b-a6cc-62698157244e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 224\n",
            "epoch 0 batch 8063 loss 0.04185900464653969\n",
            "epoch 20 batch 8063 loss 0.020469803363084793\n",
            "epoch 40 batch 8063 loss 0.02370063029229641\n",
            "epoch 60 batch 8063 loss 0.02776983380317688\n",
            "epoch 80 batch 8063 loss 0.029141241684556007\n",
            "epoch 100 batch 8063 loss 0.016244884580373764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  sum_loss = 0\n",
        "  model.eval()\n",
        "  for bi, sample in enumerate(test_dataloader):\n",
        "    X = torch.Tensor(sample[\"list\"])\n",
        "    X = X.to(torch.float32) \n",
        "    y = torch.Tensor(sample[\"mat\"])\n",
        "    y = y.to(torch.float32)\n",
        "    y_pred = model(X)           # compute model output\n",
        "    if ei == 50 and (bi % print_freq == 0):\n",
        "      pass     \n",
        "    loss = loss_fn(y, y_pred)  # calculate loss\n",
        "    sum_loss += loss\n",
        "    if bi % print_freq == 0: \n",
        "      print(f\"batch {bi} loss {loss} sum_loss {sum_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PC4LLPbVn__",
        "outputId": "4edf3418-8955-408b-d070-8a65b955f5a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.06636456400156021 sum_loss 0.06636456400156021\n",
            "batch 20 loss 0.07128918915987015 sum_loss 1.5038270950317383\n",
            "batch 40 loss 0.051932137459516525 sum_loss 2.8890087604522705\n",
            "batch 60 loss 0.09167414158582687 sum_loss 4.260381698608398\n",
            "batch 80 loss 0.0566142201423645 sum_loss 5.769962310791016\n",
            "batch 100 loss 0.07950929552316666 sum_loss 7.11648416519165\n",
            "batch 120 loss 0.0736360102891922 sum_loss 8.588957786560059\n",
            "batch 140 loss 0.06706433743238449 sum_loss 10.097452163696289\n",
            "batch 160 loss 0.07332582026720047 sum_loss 11.583922386169434\n",
            "batch 180 loss 0.07947786897420883 sum_loss 13.059700965881348\n",
            "batch 200 loss 0.0710444375872612 sum_loss 14.486085891723633\n",
            "batch 220 loss 0.060264237225055695 sum_loss 15.855779647827148\n",
            "batch 240 loss 0.06487880647182465 sum_loss 17.207229614257812\n",
            "batch 260 loss 0.08064998686313629 sum_loss 18.72004508972168\n",
            "batch 280 loss 0.0693620815873146 sum_loss 20.167112350463867\n",
            "batch 300 loss 0.06420190632343292 sum_loss 21.640092849731445\n",
            "batch 320 loss 0.07016241550445557 sum_loss 23.09482192993164\n",
            "batch 340 loss 0.05619863420724869 sum_loss 24.40513801574707\n",
            "batch 360 loss 0.07736385613679886 sum_loss 25.772445678710938\n",
            "batch 380 loss 0.05338108539581299 sum_loss 27.187450408935547\n",
            "batch 400 loss 0.07420428842306137 sum_loss 28.668468475341797\n",
            "batch 420 loss 0.0921412855386734 sum_loss 30.106191635131836\n",
            "batch 440 loss 0.06991209834814072 sum_loss 31.58916473388672\n",
            "batch 460 loss 0.09788616746664047 sum_loss 33.0700798034668\n",
            "batch 480 loss 0.06717047840356827 sum_loss 34.54529571533203\n",
            "batch 500 loss 0.08272163569927216 sum_loss 35.88554000854492\n",
            "batch 520 loss 0.07956089079380035 sum_loss 37.412391662597656\n",
            "batch 540 loss 0.04694879800081253 sum_loss 38.824832916259766\n",
            "batch 560 loss 0.06713008880615234 sum_loss 40.20903396606445\n",
            "batch 580 loss 0.09410809725522995 sum_loss 41.69172668457031\n",
            "batch 600 loss 0.09420587122440338 sum_loss 43.179325103759766\n",
            "batch 620 loss 0.0543595626950264 sum_loss 44.61836624145508\n",
            "batch 640 loss 0.09510008245706558 sum_loss 45.99591064453125\n",
            "batch 660 loss 0.0746769979596138 sum_loss 47.429931640625\n",
            "batch 680 loss 0.058805130422115326 sum_loss 48.661094665527344\n",
            "batch 700 loss 0.06191319227218628 sum_loss 49.952796936035156\n",
            "batch 720 loss 0.06327544152736664 sum_loss 51.42265319824219\n",
            "batch 740 loss 0.06731210649013519 sum_loss 52.76121139526367\n",
            "batch 760 loss 0.05553498864173889 sum_loss 54.29433059692383\n",
            "batch 780 loss 0.09074722230434418 sum_loss 55.80424118041992\n",
            "batch 800 loss 0.056438058614730835 sum_loss 57.16822814941406\n",
            "batch 820 loss 0.0714145079255104 sum_loss 58.65937042236328\n",
            "batch 840 loss 0.0871182531118393 sum_loss 60.10606384277344\n",
            "batch 860 loss 0.06483002752065659 sum_loss 61.535423278808594\n",
            "batch 880 loss 0.05366003140807152 sum_loss 62.92135238647461\n",
            "batch 900 loss 0.07011311501264572 sum_loss 64.4194564819336\n",
            "batch 920 loss 0.06490343064069748 sum_loss 65.84879302978516\n",
            "batch 940 loss 0.09261263161897659 sum_loss 67.3539047241211\n",
            "batch 960 loss 0.0826018825173378 sum_loss 68.74202728271484\n",
            "batch 980 loss 0.06276743859052658 sum_loss 70.17120361328125\n",
            "batch 1000 loss 0.09474240988492966 sum_loss 71.57989501953125\n",
            "batch 1020 loss 0.07058259099721909 sum_loss 73.06668090820312\n",
            "batch 1040 loss 0.060407958924770355 sum_loss 74.40540313720703\n",
            "batch 1060 loss 0.08216165751218796 sum_loss 75.90167236328125\n",
            "batch 1080 loss 0.08605197817087173 sum_loss 77.31886291503906\n",
            "batch 1100 loss 0.10280890017747879 sum_loss 78.83208465576172\n",
            "batch 1120 loss 0.08170246332883835 sum_loss 80.34236145019531\n",
            "batch 1140 loss 0.06653125584125519 sum_loss 81.7103500366211\n",
            "batch 1160 loss 0.06327462196350098 sum_loss 83.10181427001953\n",
            "batch 1180 loss 0.05225617438554764 sum_loss 84.49150085449219\n",
            "batch 1200 loss 0.05232469365000725 sum_loss 85.82440948486328\n",
            "batch 1220 loss 0.06125601753592491 sum_loss 87.16936492919922\n",
            "batch 1240 loss 0.0875437930226326 sum_loss 88.61495208740234\n",
            "batch 1260 loss 0.0835614949464798 sum_loss 89.91111755371094\n",
            "batch 1280 loss 0.07008948922157288 sum_loss 91.23371887207031\n",
            "batch 1300 loss 0.0462605282664299 sum_loss 92.69098663330078\n",
            "batch 1320 loss 0.0885162502527237 sum_loss 94.06988525390625\n",
            "batch 1340 loss 0.08474159985780716 sum_loss 95.45834350585938\n",
            "batch 1360 loss 0.08892685174942017 sum_loss 96.90205383300781\n",
            "batch 1380 loss 0.06655242294073105 sum_loss 98.36814880371094\n",
            "batch 1400 loss 0.05523061752319336 sum_loss 99.81787872314453\n",
            "batch 1420 loss 0.08377934992313385 sum_loss 101.22438049316406\n",
            "batch 1440 loss 0.08466733992099762 sum_loss 102.607421875\n",
            "batch 1460 loss 0.07012858986854553 sum_loss 104.0503158569336\n",
            "batch 1480 loss 0.08606693893671036 sum_loss 105.44539642333984\n",
            "batch 1500 loss 0.0873304083943367 sum_loss 106.89652252197266\n",
            "batch 1520 loss 0.09319707006216049 sum_loss 108.25083923339844\n",
            "batch 1540 loss 0.06231340020895004 sum_loss 109.82708740234375\n",
            "batch 1560 loss 0.06182127445936203 sum_loss 111.19881439208984\n",
            "batch 1580 loss 0.08096999675035477 sum_loss 112.68132781982422\n",
            "batch 1600 loss 0.06405933201313019 sum_loss 114.10421752929688\n",
            "batch 1620 loss 0.0633460059762001 sum_loss 115.48754119873047\n",
            "batch 1640 loss 0.06998487561941147 sum_loss 117.11036682128906\n",
            "batch 1660 loss 0.09238731116056442 sum_loss 118.40424346923828\n",
            "batch 1680 loss 0.07351592928171158 sum_loss 119.74127197265625\n",
            "batch 1700 loss 0.06168815493583679 sum_loss 121.05687713623047\n",
            "batch 1720 loss 0.07803811877965927 sum_loss 122.52898406982422\n",
            "batch 1740 loss 0.07291891425848007 sum_loss 123.89764404296875\n",
            "batch 1760 loss 0.06662371754646301 sum_loss 125.36097717285156\n",
            "batch 1780 loss 0.062090661376714706 sum_loss 126.66361236572266\n",
            "batch 1800 loss 0.08697349578142166 sum_loss 128.1640625\n",
            "batch 1820 loss 0.05474945902824402 sum_loss 129.5196990966797\n",
            "batch 1840 loss 0.055653613060712814 sum_loss 130.83981323242188\n",
            "batch 1860 loss 0.06865861266851425 sum_loss 132.25514221191406\n",
            "batch 1880 loss 0.11245163530111313 sum_loss 133.63339233398438\n",
            "batch 1900 loss 0.04764407128095627 sum_loss 134.95875549316406\n",
            "batch 1920 loss 0.057938240468502045 sum_loss 136.35787963867188\n",
            "batch 1940 loss 0.0676090344786644 sum_loss 137.73741149902344\n",
            "batch 1960 loss 0.07785440981388092 sum_loss 139.25106811523438\n",
            "batch 1980 loss 0.05061081051826477 sum_loss 140.62844848632812\n",
            "batch 2000 loss 0.05649169534444809 sum_loss 142.0821075439453\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = torch.Tensor([4, 3, 2, 1])\n",
        "test = test.unsqueeze(0)\n",
        "sample = test_dataset[0]\n",
        "X = sample[\"list\"]\n",
        "X = X.unsqueeze(0)\n",
        "res = model(X)\n",
        "print(X)\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9o3JCHeJRrvZ",
        "outputId": "503d0585-94e2-4183-e7f9-c3e8c95b4695"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 2., 5., 7., 0., 1., 3., 4.]])\n",
            "tensor([[[1.0000e+00, 5.6751e-34, 0.0000e+00, 1.8937e-23, 0.0000e+00,\n",
            "          0.0000e+00, 1.6461e-34, 3.6827e-10],\n",
            "         [2.9217e-10, 1.0000e+00, 3.9654e-19, 4.2872e-03, 1.9867e-14,\n",
            "          3.0894e-25, 1.6978e-29, 0.0000e+00],\n",
            "         [7.9651e-27, 2.4799e-21, 3.2102e-07, 4.4458e-01, 4.4394e-02,\n",
            "          3.5389e-08, 1.1289e-17, 0.0000e+00],\n",
            "         [7.8460e-22, 1.6747e-17, 1.3335e-09, 1.5576e-06, 3.3859e-01,\n",
            "          4.5228e-02, 2.0215e-06, 0.0000e+00],\n",
            "         [2.8726e-15, 2.9572e-12, 1.3286e-06, 1.8169e-11, 1.4406e-06,\n",
            "          4.7782e-01, 1.3665e-01, 0.0000e+00],\n",
            "         [2.4061e-12, 2.6352e-10, 2.5904e-02, 4.4076e-05, 2.6148e-09,\n",
            "          2.7311e-10, 9.2863e-01, 1.1175e-29],\n",
            "         [7.0916e-13, 3.6836e-09, 3.9730e-01, 7.8059e-02, 3.4024e-04,\n",
            "          1.7362e-06, 9.2499e-10, 4.6799e-25],\n",
            "         [1.0741e-07, 1.2476e-22, 3.0061e-02, 5.0517e-01, 7.0640e-02,\n",
            "          2.5558e-03, 7.9308e-09, 3.0764e-21],\n",
            "         [1.0634e-04, 2.9660e-02, 2.1248e-02, 1.4525e-01, 4.8550e-01,\n",
            "          7.0653e-02, 1.3032e-08, 1.8995e-16],\n",
            "         [7.4386e-04, 3.0313e-01, 2.8252e-02, 2.7352e-04, 1.8290e-01,\n",
            "          6.8135e-01, 1.4016e-08, 3.6597e-11],\n",
            "         [4.9443e-03, 6.1089e-01, 3.6514e-01, 1.3909e-02, 3.8666e-04,\n",
            "          0.0000e+00, 1.6678e-08, 3.2599e-09],\n",
            "         [7.6821e-02, 7.2196e-02, 5.9764e-01, 1.9914e-01, 9.1216e-03,\n",
            "          1.6489e-08, 6.9192e-08, 4.7625e-08],\n",
            "         [2.4718e-01, 1.0618e-01, 1.4995e-01, 2.6827e-01, 2.2627e-01,\n",
            "          5.7391e-09, 1.5822e-08, 6.7526e-07],\n",
            "         [3.6918e-01, 1.7991e-01, 2.1564e-02, 2.0693e-01, 2.8805e-01,\n",
            "          3.7521e-09, 3.1634e-08, 3.5423e-05],\n",
            "         [1.7146e-01, 2.1985e-01, 1.4640e-01, 5.2166e-04, 2.1623e-17,\n",
            "          6.5956e-08, 4.0358e-08, 9.7653e-04],\n",
            "         [9.1202e-02, 2.1451e-01, 3.0692e-01, 6.8078e-03, 0.0000e+00,\n",
            "          3.0675e-08, 1.5272e-08, 5.7650e-02],\n",
            "         [1.2144e-01, 1.7121e-01, 2.1608e-01, 2.0794e-02, 1.3446e-09,\n",
            "          1.0600e-09, 2.4015e-09, 6.2732e-01],\n",
            "         [9.0171e-02, 6.9264e-02, 5.8230e-02, 3.9703e-03, 8.8290e-09,\n",
            "          1.1513e-08, 8.5890e-08, 9.9264e-01],\n",
            "         [3.2175e-03, 1.3233e-02, 5.6014e-03, 0.0000e+00, 2.7249e-09,\n",
            "          1.8396e-08, 8.1910e-10, 9.9993e-01],\n",
            "         [2.7923e-04, 2.9613e-04, 7.4011e-06, 1.2006e-38, 4.5581e-09,\n",
            "          1.2547e-09, 1.6158e-07, 1.0000e+00],\n",
            "         [2.0575e-05, 1.5547e-05, 5.9678e-13, 1.4011e-07, 4.2449e-08,\n",
            "          7.5531e-09, 7.0946e-09, 1.0000e+00],\n",
            "         [3.3383e-08, 1.2101e-11, 1.6343e-31, 1.0999e-07, 1.3693e-08,\n",
            "          4.2305e-08, 3.3650e-09, 1.0000e+00],\n",
            "         [9.4145e-13, 3.6620e-14, 2.7122e-29, 1.7888e-08, 3.4887e-08,\n",
            "          1.5530e-08, 4.8900e-09, 1.0000e+00],\n",
            "         [1.2192e-19, 5.9803e-30, 7.5946e-10, 7.8132e-08, 7.7198e-09,\n",
            "          1.3697e-07, 1.0066e-09, 1.0000e+00],\n",
            "         [2.1111e-24, 3.0313e-20, 5.3471e-09, 1.2878e-08, 3.4418e-09,\n",
            "          1.5096e-09, 5.1546e-10, 1.0000e+00],\n",
            "         [1.5401e-07, 4.2820e-08, 4.1342e-08, 3.8624e-08, 1.0662e-08,\n",
            "          4.8947e-09, 1.5924e-09, 1.0000e+00],\n",
            "         [1.3261e-09, 2.1564e-08, 3.5733e-08, 5.4247e-08, 9.3546e-10,\n",
            "          5.8888e-09, 1.2884e-08, 1.0000e+00],\n",
            "         [4.5355e-09, 4.0586e-09, 2.4638e-08, 7.9806e-08, 2.5384e-09,\n",
            "          6.6012e-10, 2.2070e-09, 1.0000e+00]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(res, dim=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArbZvT2yIOSE",
        "outputId": "2654efaf-eb8f-448f-d31b-cf7a1506113c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 1.0043, 0.4890, 0.3838, 0.6145, 0.9546, 0.4757, 0.6084, 0.7524,\n",
              "         1.1967, 0.9953, 0.9549, 0.9978, 1.0657, 0.5392, 0.6771, 1.1568, 1.2143,\n",
              "         1.0220, 1.0006, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
              "         1.0000]], grad_fn=<SumBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Square(nn.Module):\n",
        "    \"\"\" Custom Linear layer but mimics a standard linear layer \"\"\"\n",
        "    def __init__(self, size_in, size_out):\n",
        "        super().__init__()\n",
        "        self.size_in, self.size_out = size_in, size_out\n",
        "        weights = torch.Tensor(size_out, size_in)\n",
        "        self.weights = nn.Parameter(weights)  # nn.Parameter is a Tensor that's a module parameter.\n",
        "        bias = torch.Tensor(size_out)\n",
        "        self.bias = nn.Parameter(bias)\n",
        "\n",
        "        # initialize weights and biases\n",
        "        nn.init.kaiming_uniform_(self.weights, a=math.sqrt(5)) # weight init\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weights)\n",
        "        bound = 1 / math.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n",
        "\n",
        "    def forward(self, x):\n",
        "        w_times_x= torch.mm(x, self.weights.t())\n",
        "        # \n",
        "        print(f\" x shape {x.shape} weights shape {self.weights.shape} res shape {w_timex_x.shape}\")\n",
        "        return torch.add(w_times_x, self.bias) "
      ],
      "metadata": {
        "id": "QTATN8dJHgo3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SSgIJZTFLBjt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}